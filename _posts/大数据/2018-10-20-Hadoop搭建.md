---
tags: [大数据]
---
## <center>Hadoop 环境搭建（基于Hadoop 2.0）</center>

#### 1.上传Hadoop包

将包上传到/usr/lcoal目录下，并用`tar -zxvf `命令解压

#### 2.1单节点配置

安装单节点的Hadoop不需要任何配置，这种方式经常用来调试

####2.2伪分布式配置

>**预备工作**
>
>* 修改静态IP
>* 修改主机名和修改hosts文件（windows系统 & CentOS都要改）
>* 关闭IPV6
>* 关闭防火墙

所有的配置文件都在解压后的文件夹的`/usr/local/hadoop/etc/hadoop`目录下

- 修改hadoop-env.sh配置

> 因为hadoop依赖JDK，这里修改是为了配置JAVA_HOME
>
> ![](http://img.blog.csdn.net/20180330152030372)

- 修改core-site.xml

> 配置HDFS的NameNode地址和Hadoop运行时产生文件的存放目录
>
> ![](http://img.blog.csdn.net/20180401193406384)
>
> ```xml
> <configuration>
>   <property>
>     <name>fs.defaultFS</name>
>     <value>hdfs://hadoop-test:9000</value> <!-- 要去修改hosts文件后才能这样设置 -->
>   </property>
>
>   <property>
>     <name>hadoop.tmp.dir</name>
>     <value>/usr/local/hadoop-2.6.5/tmp</value>
>   </property>
> </configuration>
> ```

- 修改hdfs-site.xml

> 配置HDFS中副本的数量（既是，一个文件总共的数量）
>
> ![](http://img.blog.csdn.net/20180331174624406)
>
> ```xml
> <configuration>
>   <property>
>     <name>dfs.replication</name>
>     <value>1</value>
>   </property>
> </configuration>
> ```
>
> 因为我们搭建的伪分布式，只有一台机器，所以只保存一份。一般默认值是3

- 修改mapred-site.xml

> 如果没有mapred-site.xml，可以将mapred-site.xml.template的内容直接复制过来
>
> ![](http://img.blog.csdn.net/20180331175620936)
>
> ```xml
> <configuration>
>   <property>
>     <name>mapreduce.framework.name</name>
>     <value>yarn</value>
>   </property>
> </configuration>
> ```
>
> 指定Hadoop以后运行在yarn上

- 修改yarn-site.xml

> ![](http://img.blog.csdn.net/20180401193244446)
>
> ```xml
> <configuration>
>   <property>
>     <name>yarn.nodemanager.aux-services</name>
>     <value>mapreduce_shuffle</value>
>   </property>
>   
>   <property>
>     <name>yarn.resourcemanager.hostname</name>
>     <value>hadoop-test</value> <!-- 要去修改hosts文件后才能这样设置 -->
>   </property>
> </configuration>
> ```
>
> 指定NodeManager获取数据的方式是shuffle，指定resourcemanager的地址

#### 3.运行hadoop

**①将hadoop的bin添加到环境变量**

修改`/etc/profile`文件，添加环境变量配置

![](https://img-blog.csdn.net/20180331214113609)

```xml
export HADOOP_HOME=/usr/local/hadoop
exoprt PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin

// 测试
hadoop verison
yarn version
```

接着，使用`source /etc/profile`命令让修改后的配置文件立即生效

**②初始化HDFS（只有首次启动需要格式化）**

~~`hadoop namenode -format`~~（过时）

现在使用：`hdfs namenode -format`

![](https://img-blog.csdn.net/20180331215151981)

**③启动hadoop**

* 配置免密码登陆：

`cd ~`，然后`cd .ssh/`

输入`ssh-keygen -t rsa`，接着4个回车

![](http://img.blog.csdn.net/20180331225443474)

这样.ssh文件夹下就生成了公钥：`id_rsa.pub`私钥：`id_rsa`

将公钥拷贝到自己的指定目录下，实现自我免登录`cp id_rsa.pub authorized_keys`或者用命令`ssh-copy-id 目的服务器IP`

* 不配置免密码登陆，密码为服务器密码



`/usr/local/hadoop/sbin`目录下，进行启动

~~`./start-all.sh`~~(过时)

现在使用：`./start-dfs.sh`先启动HDFS，`./start-yarn.sh`再启动YARN

用jps命令可以看到已经启动的进程，验证是否成功

![](http://img.blog.csdn.net/20180401171348489)

每次在关机之前记得关闭hadoop和yarn

~~`./stop-all.sh`~~(过时)

现在使用：`./stop-dfs.sh`和`./stop-yarn.sh`

#### 4.测试Hadoop

http://192.168.72.129:50070  (HDFS管理界面)

![](http://img.blog.csdn.net/20180401171810203)

http://192.168.72.129:8088 （YARN管理界面）

![](http://img.blog.csdn.net/20180401193500551)

文件上传测试：

`hadoop fs -put /usr/local/temp/hadoop入门实战手册.rar hdfs://localhost:9000/doc`

![](http://img.blog.csdn.net/20180401202639511)

#### 5.word count 入门程序

**目标**：统计一个文本中每个单词出现的次数，文本words.txt如下

>hello tom
>hello kity
>hello bob
>hello Flynn

**工程结构**：

![](http://img.blog.csdn.net/20180414165635580)

**代码**：

`WordCount.java`

```java
import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		// 构建一个job对象，以后的每次任务被抽象为了job对象
		Job job = Job.getInstance(new Configuration());
		
		// 注意：一定要将main方法所在的类设置进去
		job.setJarByClass(WordCount.class);
		
		// 设置mapper属性
		job.setMapperClass(WCMapper.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(LongWritable.class);
		// 这个文件是在HDFS上
		FileInputFormat.setInputPaths(job, new Path("/words.txt"));
		
		// 设置reducer属性
		job.setReducerClass(WCReducer.class);
		job.setOutputKeyClass(Text.class); // 可以是map的输出，也可以是reduce的输出，二者通用这个函数
		job.setOutputValueClass(LongWritable.class);
		FileOutputFormat.setOutputPath(job, new Path("/wcout"));
		
		// 提交任务
		job.waitForCompletion(true);
	}
}
```

`WCMapper.java`

```java
import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

/**
 * 
 * LongWritable 输入的key
 * Text 输入的value
 * 
 * Text 输出的key
 * LongWritable 输出的value
 *
 */
public class WCMapper extends Mapper<LongWritable, Text, Text, LongWritable>{

	@Override
	protected void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {
		// 接收数据v1
		String line = value.toString();
		// 切分数据
		String[] words = line.split(" ");
		// 循环
		for(String w : words){
			// 单词出现一次就记一个1，然后输出key2、value2
			context.write(new Text(w), new LongWritable(1)); // new Text(w),new LongWritable(1) 是为了类型转换
		}
	}
}
```

`WCReducer.java`

```java
import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

/**
 * 参数分别是输入key、value和输出key、value
 *
 */
public class WCReducer extends Reducer<Text, LongWritable, Text, LongWritable>{
	
	@Override
	protected void reduce(Text key, Iterable<LongWritable> values, Context context)
			throws IOException, InterruptedException {
		// 定义计数器
		long counter = 0;
		// 循环统计
		for(LongWritable l : values){
			counter += l.get();
		}
		// 输出结果
		context.write(key, new LongWritable(counter));
	}
}
```

**运行**：

* 首先上传words.txt到HDFS

![](http://img.blog.csdn.net/20180414171317661)

* 用hadoop执行项目的jar包

执行命令`hadoop jar wc.jar`

* 查看统计结果

![](http://img.blog.csdn.net/20180414171849165)

#### 附：可能遇到的问题

1. `WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform`<br/>这个问题是因为：apache官网提供的二进制包，里面的native库，是32位的，而我们的服务器是64位的。<br/>解决方案：在64位CentOS上编译 Hadoop源码，然后替换掉`/usr/local/hadoop/lib/native`文件夹

2. 如果发现节点没有启动，就去`hadoop/logs`目录下去查看日志输出

3. 访问HDFS出错<br/>core-site.xml中配置`ip:端口`无法启动dataNode，而配置`主机名:端口`就可以。但是，问题在于如果配置主机名localhost，在访问HDFS Web时，域名为localhost就访问不到，需要手动填写IP（待后续探究。。。。）

   

   ## <center>Hadoop集群搭建（基于CDH5）</center>

1.企业级平台搭建过程

**选择CDH的原因**



**测试环境**

>| 参数名  | 参数值           |
>| ---- | ------------- |
>| 机器数量 | 5-10（台）       |
>| 内存   | 24G或32G（推荐大小） |
>| 硬盘   | 4TB           |
>| CPU  | 6核            |
>| 网卡   | 万兆网卡、网线       |
>


**生产环境**

>**小型集群的硬件参数**
>
>20台以下
>
>**中型集群的硬件参数**
>
>50台以下
>
>**中型集群的硬件参数**
>
>50台以上，百度的一个Spark集群两千台



#### 2.Zookeeper集群搭建

>**小型集群**
>
>3-5台
>
>**中型集群**
>
>5-7台
>
>**大型集群**
>
>奇数台

注：

NameNode每存一百万个文件需要1G的内存，所以NameNode需要8G-12G的内存；

DataNode主要是数据的读写，一般4G或6G；

YARN负责任务调度，一般2G到4G；

NodeManager一般2G就够了；

一个Map任务需要需要一核；



#### 3搭建步骤

- 安装

1. tar包安装：http://archive.cloudera.com/cdh5/cdh/5/
2. rpm包安装：http://archive.cloudera.com/cdh5/redhat/7/x86_64/cdh/
3. parcels包安装（CDH4.1.2之后才有，安装极为方便，使用公司很多，也是**官方推荐**的安装方式）：用Cloudera Manager来进行安装

- 

注：磁盘阵列推荐JOBD；IP地址尽可能在同一个网段；主机名的设置：bigdata-cdh01.bookcycle.com

主机名的改法：hostname 主机名（暂时） 

vi /etc/sysconfig/network  在文件中修改主机名（下次重启后永久生效）；

centos7的修改方法：hostnamectl set-hostname 主机名



**首先**，修改主机名（修改所有主机的主机名）格式：bigdata-cdh01.bookcycle.cn

**其次**，修改映射

vi 、etc/hosts

hosts文件的第二行 ::1几乎没有什么用，可以删删除掉，但是第一行不要全部删除调,然后调整位置



IP与 主机名映射

vi etc/hosts

将01机的hosts文件内容改为如下：

```xml
127.0.0.1  localhost.localdomain localhost

## BigData CDH 5.x
192.168.38.128  bigdata-cdh01.bookcycle.cn      bigdata-cdh01
192.168.38.129  bigdata-cdh02.bookcycle.cn      bigdata-cdh02
192.168.38.130  bigdata-cdh03.bookcycle.cn      bigdata-cdh03
192.168.38.131  bigdata-cdh04.bookcycle.cn      bigdata-cdh04
192.168.38.132  bigdata-cdh05.bookcycle.cn      bigdata-cdh05
```

其余的机器hosts文件同上



注意：客户端的机器也需要添加这些映射

C:\Windows\System32\drivers\etc\hosts



**然后**，创建普通用户，用于安装软件（集群中的所有主机的普通用户一定要一致）



**切换账户到普通用户**

su命令可以直接切换到root

（也可以为普通用户配置dalian权限 `chmod u+w /etc/sudoers`, 然后 `vi /etc/sudoers`，增加内容 flynn ALL=(root)NOPASSWD：ALL，最后收回权限`chmod u-w /etc/sudoers`） 其中的flynn是创建用户的用户名 

禁用IPV6，执行命令`echo "alia net-pf-10 off" >> /etc/modprobe.d/dist.conf`

`echo "alia ipv6 off" >> /etc/modprobe.d/dist.conf`

**防火墙配置**

关闭防火墙或者放开要用的端口



**装NTP，保证时间同步**

所有节点安装相关组件：`yum install ntp`。完成后，配置开机启动：`chkconfig ntpd on`,检查是否设置成功：`chkconfig --list ntpd`其中2-5为on状态就代表成功。

##<center>Spark环境搭建（基于Spark 1.6）</center>



#### 1.安装scala

* 下载Scala

https://www.scala-lang.org/download/2.12.6.html  scala-2.12.6.tgz

* 解压到/usr/local

`tar-zxvf scala-2.12.6.tgz`

* 设置环境变量

`vi /etc/profile`

```
添加SCALA_HOME并增加PATH值
export SCALA_HOME=/usr/local/scala-2.12.6
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$SCALA_HOME/bin

刷新怕配置
source /etc/profile
```

* 检查Scala是否安装成功

```
[root@hadoop-test local]# scala -version
Scala code runner version 2.12.6 -- Copyright 2002-2018, LAMP/EPFL and Lightbend, Inc.
```

#### 2.配置Spark

`tar -zxvf spark-1.6.0-bin-hadoop2.6.tgz`

```
打开spark文件夹
cd spark-1.6.0-bin-hadoop2.6
把缓存的文件spark-env.sh.template改为spark识别的文件spark-env.sh
cd conf
cp spark-env.sh.template spark-env.sh
修改spark-env.sh文件
vi /spark-env.sh 在末尾加入，如下内容(波浪线包围部分)：
​~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
JAVA_HOME=/usr/local/jdk1.8.0_121
HADOOP_HOME=/usr/local/hadoop-2.6.5
SCALA_HOME=/usr/local/scala-2.12.6

export HADOOP_CONF_DIR=/usr/local/hadoop-2.6.5/etc/hadoop

export SPARK_MASTER_IP=hadoop-test  （可以省略） 
export SPARK_WORKER_MEMORY=4g
export SPARK_WORKER_CORES=2
export SPARK_WORKER_INSTANCES=1
​~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
变量说明 
- JAVA_HOME：Java安装目录 
- SCALA_HOME：Scala安装目录 
- HADOOP_HOME：hadoop安装目录 
- HADOOP_CONF_DIR：hadoop集群的配置文件的目录 
- SPARK_MASTER_IP：spark集群的Master节点的ip地址 
- SPARK_WORKER_MEMORY：每个worker节点能够最大分配给exectors的内存大小 
- SPARK_WORKER_CORES：每个worker节点所占有的CPU核数目 
- SPARK_WORKER_INSTANCES：每台机器上开启的worker节点的数目

启动单机版Spark
cd sbin
./start-all.sh
停止单机版Spark
./stop-all.sh

使用Spark Shell
./bin/spark-shell
```

进入WebUI看Spark

http://hadoop-test:8080















